{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN & Word Embeddings for SEC Filings to Predict Returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNs are commonly applied to various natural language processing tasks. We've already encountered sentiment analysis using text data in part three of this book.\n",
    "\n",
    "We are now going to apply an RNN model to SEC filings to learn custom word embeddings (see Chapter 16) and predict the returns over the week after the filing date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:15:33.518582Z",
     "start_time": "2021-02-23T17:15:33.516662Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T18:29:18.731933Z",
     "start_time": "2021-02-23T18:29:18.705423Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "from collections import Counter\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "import yfinance as yf\n",
    "\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (Dense, GRU, Bidirectional,\n",
    "                                     Embedding, BatchNormalization, Dropout)\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsoluteError\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:15:35.559477Z",
     "start_time": "2021-02-23T17:15:35.526207Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print('Using GPU')\n",
    "    tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
    "else:\n",
    "    print('Using CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:15:36.443787Z",
     "start_time": "2021-02-23T17:15:36.441899Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:15:36.619307Z",
     "start_time": "2021-02-23T17:15:36.616921Z"
    }
   },
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:15:36.755607Z",
     "start_time": "2021-02-23T17:15:36.752932Z"
    }
   },
   "outputs": [],
   "source": [
    "def format_time(t):\n",
    "    m, s = divmod(t, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    return f'{h:02.0f}:{m:02.0f}:{s:02.0f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:15:36.909263Z",
     "start_time": "2021-02-23T17:15:36.907002Z"
    }
   },
   "outputs": [],
   "source": [
    "deciles = np.arange(.1, 1, .1).round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get stock price data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:15:40.900853Z",
     "start_time": "2021-02-23T17:15:40.898690Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = Path('..', 'data', 'sec-filings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:15:42.542441Z",
     "start_time": "2021-02-23T17:15:42.539369Z"
    }
   },
   "outputs": [],
   "source": [
    "results_path = Path('results', 'sec-filings')\n",
    "\n",
    "selected_section_path = results_path / 'ngrams_1'\n",
    "ngram_path = results_path / 'ngrams'\n",
    "vector_path = results_path / 'vectors'\n",
    "\n",
    "for path in [vector_path, selected_section_path, ngram_path]:\n",
    "    if not path.exists():\n",
    "        path.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get filing info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:15:45.140119Z",
     "start_time": "2021-02-23T17:15:45.066220Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '..\\\\data\\\\sec-filings\\\\filing_index.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-d4140df799d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m filing_index = (pd.read_csv(data_path / 'filing_index.csv',\n\u001b[0m\u001b[0;32m      2\u001b[0m                             parse_dates=['DATE_FILED'])\n\u001b[0;32m      3\u001b[0m                 .rename(columns=str.lower))\n\u001b[0;32m      4\u001b[0m \u001b[0mfiling_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml4t\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml4t\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml4t\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml4t\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml4t\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml4t\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \"\"\"\n\u001b[1;32m-> 1362\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml4t\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '..\\\\data\\\\sec-filings\\\\filing_index.csv'"
     ]
    }
   ],
   "source": [
    "filing_index = (pd.read_csv(data_path / 'filing_index.csv',\n",
    "                            parse_dates=['DATE_FILED'])\n",
    "                .rename(columns=str.lower))\n",
    "filing_index.index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:15:45.395434Z",
     "start_time": "2021-02-23T17:15:45.381675Z"
    }
   },
   "outputs": [],
   "source": [
    "filing_index.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:15:47.808079Z",
     "start_time": "2021-02-23T17:15:47.792905Z"
    }
   },
   "outputs": [],
   "source": [
    "filing_index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:15:47.981965Z",
     "start_time": "2021-02-23T17:15:47.974876Z"
    }
   },
   "outputs": [],
   "source": [
    "filing_index.ticker.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:15:49.960520Z",
     "start_time": "2021-02-23T17:15:49.953116Z"
    }
   },
   "outputs": [],
   "source": [
    "filing_index.date_filed.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download stock price data using Yfinance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`yfinance` can be unstable so that connections drop; if you experience this you may want to store intermediate results so you don't have to start over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:23:48.428286Z",
     "start_time": "2021-02-23T17:15:57.201324Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yf_data, missing = [], []\n",
    "for i, (symbol, dates) in enumerate(filing_index.groupby('ticker').date_filed, 1):\n",
    "    \n",
    "    if i % 250 == 0:\n",
    "        print(i, len(yf_data), len(set(missing)), flush=True)\n",
    "    \n",
    "    ticker = yf.Ticker(symbol)\n",
    "    for filing, date in dates.to_dict().items():\n",
    "        start = date - timedelta(days=93)\n",
    "        end = date + timedelta(days=31)\n",
    "        df = ticker.history(start=start, end=end)\n",
    "        if df.empty:\n",
    "            missing.append(symbol)\n",
    "        else:\n",
    "            yf_data.append(df.assign(ticker=symbol, filing=filing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:23:48.433407Z",
     "start_time": "2021-02-23T17:15:57.204Z"
    }
   },
   "outputs": [],
   "source": [
    "yf_data = pd.concat(yf_data).rename(columns=str.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:23:48.434099Z",
     "start_time": "2021-02-23T17:15:57.208Z"
    }
   },
   "outputs": [],
   "source": [
    "yf_data.to_hdf(results_path / 'sec_returns.h5', 'data/yfinance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yf_data = pd.read_hdf(results_path / 'sec_returns.h5', 'data/yfinance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:23:48.434772Z",
     "start_time": "2021-02-23T17:15:57.212Z"
    }
   },
   "outputs": [],
   "source": [
    "yf_data.ticker.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:23:48.435514Z",
     "start_time": "2021-02-23T17:15:57.216Z"
    }
   },
   "outputs": [],
   "source": [
    "yf_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get (some) missing prices from Quandl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:23:48.436249Z",
     "start_time": "2021-02-23T17:15:57.220Z"
    }
   },
   "outputs": [],
   "source": [
    "to_do = (filing_index.loc[~filing_index.ticker.isin(yf_data.ticker.unique()), \n",
    "                          ['ticker', 'date_filed']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:23:48.436962Z",
     "start_time": "2021-02-23T17:15:57.224Z"
    }
   },
   "outputs": [],
   "source": [
    "to_do.date_filed.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:23:48.437672Z",
     "start_time": "2021-02-23T17:15:57.227Z"
    }
   },
   "outputs": [],
   "source": [
    "quandl_tickers = (pd.read_hdf('../data/assets.h5', 'quandl/wiki/prices')\n",
    "                  .loc[idx['2012':, :], :]\n",
    "                  .index.unique('ticker'))\n",
    "quandl_tickers = list(set(quandl_tickers).intersection(set(to_do.ticker)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:23:48.438476Z",
     "start_time": "2021-02-23T17:15:57.231Z"
    }
   },
   "outputs": [],
   "source": [
    "len(quandl_tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:23:48.438946Z",
     "start_time": "2021-02-23T17:15:57.234Z"
    }
   },
   "outputs": [],
   "source": [
    "to_do = filing_index.loc[filing_index.ticker.isin(quandl_tickers), ['ticker', 'date_filed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:23:48.439410Z",
     "start_time": "2021-02-23T17:15:57.238Z"
    }
   },
   "outputs": [],
   "source": [
    "to_do.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:23:48.439874Z",
     "start_time": "2021-02-23T17:15:57.241Z"
    }
   },
   "outputs": [],
   "source": [
    "ohlcv = ['adj_open', 'adj_high', 'adj_low', 'adj_close', 'adj_volume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:23:48.440346Z",
     "start_time": "2021-02-23T17:15:57.244Z"
    }
   },
   "outputs": [],
   "source": [
    "quandl = (pd.read_hdf('../data/assets.h5', 'quandl/wiki/prices')\n",
    "          .loc[idx['2012': , quandl_tickers], ohlcv]\n",
    "          .rename(columns=lambda x: x.replace('adj_', '')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:23:48.440795Z",
     "start_time": "2021-02-23T17:15:57.247Z"
    }
   },
   "outputs": [],
   "source": [
    "quandl.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:23:48.441361Z",
     "start_time": "2021-02-23T17:15:57.250Z"
    }
   },
   "outputs": [],
   "source": [
    "quandl_data = []\n",
    "for i, (symbol, dates) in enumerate(to_do.groupby('ticker').date_filed, 1):\n",
    "    if i % 100 == 0:\n",
    "        print(i, end=' ', flush=True)\n",
    "    for filing, date in dates.to_dict().items():\n",
    "        start = date - timedelta(days=93)\n",
    "        end = date + timedelta(days=31)\n",
    "        quandl_data.append(quandl.loc[idx[start:end, symbol], :].reset_index('ticker').assign(filing=filing))\n",
    "quandl_data = pd.concat(quandl_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:23:48.442102Z",
     "start_time": "2021-02-23T17:15:57.254Z"
    }
   },
   "outputs": [],
   "source": [
    "quandl_data.to_hdf(results_path / 'sec_returns.h5', 'data/quandl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine, clean and persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:23:48.443541Z",
     "start_time": "2021-02-23T17:15:57.258Z"
    }
   },
   "outputs": [],
   "source": [
    "data = (pd.read_hdf(results_path / 'sec_returns.h5', 'data/yfinance')\n",
    "        .drop(['dividends', 'stock splits'], axis=1)\n",
    "        .append(pd.read_hdf(results_path / 'sec_returns.h5',\n",
    "                            'data/quandl')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:23:48.444144Z",
     "start_time": "2021-02-23T17:15:57.261Z"
    }
   },
   "outputs": [],
   "source": [
    "data = data.loc[:, ['filing', 'ticker', 'open', 'high', 'low', 'close', 'volume']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:23:48.444903Z",
     "start_time": "2021-02-23T17:15:57.265Z"
    }
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:23:48.445594Z",
     "start_time": "2021-02-23T17:15:57.268Z"
    }
   },
   "outputs": [],
   "source": [
    "data[['filing', 'ticker']].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:23:48.446239Z",
     "start_time": "2021-02-23T17:15:57.271Z"
    }
   },
   "outputs": [],
   "source": [
    "data.to_hdf(results_path / 'sec_returns.h5', 'prices')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy filings with stock price data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:28:17.471873Z",
     "start_time": "2021-02-23T17:28:17.336270Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_hdf(results_path / 'sec_returns.h5', 'prices')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:28:18.407009Z",
     "start_time": "2021-02-23T17:28:18.393588Z"
    }
   },
   "outputs": [],
   "source": [
    "filings_with_data = data.filing.unique()\n",
    "len(filings_with_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove short and long sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:28:23.473629Z",
     "start_time": "2021-02-23T17:28:23.471493Z"
    }
   },
   "outputs": [],
   "source": [
    "min_sentence_length = 5\n",
    "max_sentence_length = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:30:38.587875Z",
     "start_time": "2021-02-23T17:28:23.475274Z"
    }
   },
   "outputs": [],
   "source": [
    "sent_length = Counter()\n",
    "for i, idx in enumerate(filings_with_data, 1):\n",
    "    if i % 500 == 0:\n",
    "        print(i, end=' ', flush=True)\n",
    "    text = pd.read_csv(data_path / 'selected_sections' / f'{idx}.csv').text\n",
    "    sent_length.update(text.str.split().str.len().tolist())\n",
    "    text = text[text.str.split().str.len().between(min_sentence_length, max_sentence_length)]\n",
    "    text = '\\n'.join(text.tolist())\n",
    "    with (selected_section_path / f'{idx}.txt').open('w') as f:\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:30:38.592639Z",
     "start_time": "2021-02-23T17:30:38.588902Z"
    }
   },
   "outputs": [],
   "source": [
    "sent_length = pd.Series(dict(sent_length.most_common()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:30:39.079937Z",
     "start_time": "2021-02-23T17:30:38.593805Z"
    }
   },
   "outputs": [],
   "source": [
    "with sns.axes_style(\"white\"):\n",
    "    sent_length.sort_index().cumsum().div(sent_length.sum()).loc[5:51].plot.bar(figsize=(12, 4), rot=0)\n",
    "    sns.despine();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:30:39.552221Z",
     "start_time": "2021-02-23T17:30:39.081381Z"
    }
   },
   "outputs": [],
   "source": [
    "with sns.axes_style(\"white\"):\n",
    "    sent_length.sort_index().loc[:50].plot.bar(figsize=(14, 4))\n",
    "    sns.despine();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create bi- and trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all filings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:30:44.634112Z",
     "start_time": "2021-02-23T17:30:39.553298Z"
    }
   },
   "outputs": [],
   "source": [
    "files = selected_section_path.glob('*.txt')\n",
    "texts = [f.read_text() for f in files]\n",
    "unigrams = ngram_path / 'ngrams_1.txt'\n",
    "unigrams.write_text('\\n'.join(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:30:45.954454Z",
     "start_time": "2021-02-23T17:30:44.635150Z"
    }
   },
   "outputs": [],
   "source": [
    "texts = unigrams.read_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes quite some time; last attempt was 30 min per iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T18:27:21.916360Z",
     "start_time": "2021-02-23T17:30:45.955444Z"
    }
   },
   "outputs": [],
   "source": [
    "n_grams = []\n",
    "start = time()\n",
    "for i, n in enumerate([2, 3]):\n",
    "    sentences = LineSentence(ngram_path / f'ngrams_{n-1}.txt')\n",
    "    phrases = Phrases(sentences=sentences,\n",
    "                      min_count=25,  # ignore terms with a lower count\n",
    "                      threshold=0.5,  # accept phrases with higher score\n",
    "                      max_vocab_size=4000000,  # prune of less common words to limit memory use\n",
    "                      delimiter=b'_',  # how to join ngram tokens\n",
    "                      scoring='npmi')\n",
    "\n",
    "    s = pd.DataFrame([[k.decode('utf-8'), v] for k, v in phrases.export_phrases(sentences)], \n",
    "                     columns=['phrase', 'score']).assign(length=n)\n",
    "\n",
    "    n_grams.append(s.groupby('phrase').score.agg(['mean', 'size']))\n",
    "    print(n_grams[-1].nlargest(5, columns='size'))\n",
    "    \n",
    "    grams = Phraser(phrases)\n",
    "    sentences = grams[sentences]\n",
    "    (ngram_path / f'ngrams_{n}.txt').write_text('\\n'.join([' '.join(s) for s in sentences]))\n",
    "    \n",
    "    src_dir = results_path / f'ngrams_{n-1}'\n",
    "    target_dir = results_path / f'ngrams_{n}'\n",
    "    if not target_dir.exists():\n",
    "        target_dir.mkdir()\n",
    "    \n",
    "    for f in src_dir.glob('*.txt'):\n",
    "        text = LineSentence(f)\n",
    "        text = grams[text]\n",
    "        (target_dir / f'{f.stem}.txt').write_text('\\n'.join([' '.join(s) for s in text]))\n",
    "    print('\\n\\tDuration: ', format_time(time() - start))\n",
    "\n",
    "n_grams = pd.concat(n_grams).sort_values('size', ascending=False)          \n",
    "n_grams.to_parquet(results_path / 'ngrams.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T18:27:21.989029Z",
     "start_time": "2021-02-23T18:27:21.918907Z"
    }
   },
   "outputs": [],
   "source": [
    "n_grams.groupby(n_grams.index.str.replace('_', ' ').str.count(' ')).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert filings to integer sequences based on token count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T18:27:24.927153Z",
     "start_time": "2021-02-23T18:27:21.991296Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences = (ngram_path / 'ngrams_3.txt').read_text().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T18:27:24.930003Z",
     "start_time": "2021-02-23T18:27:24.928123Z"
    }
   },
   "outputs": [],
   "source": [
    "n = len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T18:28:05.317863Z",
     "start_time": "2021-02-23T18:27:24.930922Z"
    }
   },
   "outputs": [],
   "source": [
    "token_cnt = Counter()\n",
    "for i, sentence in enumerate(sentences, 1):\n",
    "    if i % 500000 == 0:\n",
    "        print(f'{i/n:.1%}', end=' ', flush=True)\n",
    "    token_cnt.update(sentence.split())\n",
    "token_cnt = pd.Series(dict(token_cnt.most_common()))\n",
    "token_cnt = token_cnt.reset_index()\n",
    "token_cnt.columns = ['token', 'n']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T18:28:05.369063Z",
     "start_time": "2021-02-23T18:28:05.318708Z"
    }
   },
   "outputs": [],
   "source": [
    "token_cnt.to_parquet(results_path / 'token_cnt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T18:28:05.384586Z",
     "start_time": "2021-02-23T18:28:05.370044Z"
    }
   },
   "outputs": [],
   "source": [
    "token_cnt.n.describe(deciles).apply(lambda x: f'{x:,.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T18:28:05.413267Z",
     "start_time": "2021-02-23T18:28:05.385647Z"
    }
   },
   "outputs": [],
   "source": [
    "token_cnt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T18:28:05.433004Z",
     "start_time": "2021-02-23T18:28:05.415213Z"
    }
   },
   "outputs": [],
   "source": [
    "token_cnt.nlargest(10, columns='n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T18:28:05.987147Z",
     "start_time": "2021-02-23T18:28:05.434330Z"
    }
   },
   "outputs": [],
   "source": [
    "token_cnt.sort_values(by=['n', 'token'], ascending=[False, True]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T18:28:06.526810Z",
     "start_time": "2021-02-23T18:28:05.988291Z"
    }
   },
   "outputs": [],
   "source": [
    "token_by_freq = token_cnt.sort_values(by=['n', 'token'], ascending=[False, True]).token\n",
    "token2id = {token: i for i, token in enumerate(token_by_freq, 3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T18:28:06.530326Z",
     "start_time": "2021-02-23T18:28:06.527670Z"
    }
   },
   "outputs": [],
   "source": [
    "len(token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T18:28:06.543726Z",
     "start_time": "2021-02-23T18:28:06.531469Z"
    }
   },
   "outputs": [],
   "source": [
    "for token, i in token2id.items():\n",
    "    print(token, i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T18:29:24.968098Z",
     "start_time": "2021-02-23T18:29:24.962021Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_sequences(min_len=100, max_len=20000, num_words=25000, oov_char=2):\n",
    "    if not vector_path.exists():\n",
    "        vector_path.mkdir()\n",
    "    seq_length = {}\n",
    "    skipped = 0\n",
    "    for i, f in tqdm(enumerate((results_path / 'ngrams_3').glob('*.txt'), 1)):\n",
    "        file_id = f.stem\n",
    "        text = f.read_text().split('\\n')\n",
    "        vector = [token2id[token] if token2id[token] + 2 < num_words else oov_char \n",
    "                  for line in text \n",
    "                  for token in line.split()]\n",
    "        vector = vector[:max_len]\n",
    "        if len(vector) < min_len:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        seq_length[int(file_id)] = len(vector)\n",
    "        np.save(vector_path / f'{file_id}.npy', np.array(vector))\n",
    "    seq_length = pd.Series(seq_length)\n",
    "    return seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T18:30:25.139961Z",
     "start_time": "2021-02-23T18:29:25.096611Z"
    }
   },
   "outputs": [],
   "source": [
    "seq_length = generate_sequences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T18:31:59.650371Z",
     "start_time": "2021-02-23T18:31:59.613544Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.Series(seq_length).to_csv(results_path / 'seq_length.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T18:31:59.780990Z",
     "start_time": "2021-02-23T18:31:59.775678Z"
    }
   },
   "outputs": [],
   "source": [
    "seq_length.describe(deciles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T18:32:00.048040Z",
     "start_time": "2021-02-23T18:32:00.044991Z"
    }
   },
   "outputs": [],
   "source": [
    "seq_length.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T18:32:04.474609Z",
     "start_time": "2021-02-23T18:32:02.368930Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=3, figsize=(18,5))\n",
    "token_cnt.n.plot(logy=True, logx=True, ax=axes[0], title='Token Frequency (log-log scale)')\n",
    "sent_length.sort_index().loc[:50].plot.bar(ax=axes[1], rot=0, title='Sentence Length')\n",
    "\n",
    "n=5\n",
    "ticks = axes[1].xaxis.get_ticklocs()\n",
    "ticklabels = [l.get_text() for l in axes[1].xaxis.get_ticklabels()]\n",
    "axes[1].xaxis.set_ticks(ticks[n-1::n])\n",
    "axes[1].xaxis.set_ticklabels(ticklabels[n-1::n])\n",
    "axes[1].set_xlabel('Sentence Length')\n",
    "\n",
    "sns.distplot(seq_length, ax=axes[2], bins=50)\n",
    "axes[0].set_ylabel('Token Frequency')\n",
    "axes[0].set_xlabel('Token ID')\n",
    "\n",
    "axes[2].set_xlabel('# Words per Filing')\n",
    "axes[2].set_title('Filing Length Distribution')\n",
    "\n",
    "fig.suptitle('Corpus Stats', fontsize=13)\n",
    "sns.despine()\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=.85)\n",
    "fig.savefig(results_path / 'sec_seq_len', dpi=300);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T18:32:04.520847Z",
     "start_time": "2021-02-23T18:32:04.476018Z"
    }
   },
   "outputs": [],
   "source": [
    "files = vector_path.glob('*.npy')\n",
    "filings = sorted([int(f.stem) for f in files])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Model Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create weekly forward returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T18:32:43.198031Z",
     "start_time": "2021-02-23T18:32:43.009584Z"
    }
   },
   "outputs": [],
   "source": [
    "prices = pd.read_hdf(results_path / 'sec_returns.h5', 'prices')\n",
    "prices.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T18:33:19.708318Z",
     "start_time": "2021-02-23T18:32:43.199003Z"
    }
   },
   "outputs": [],
   "source": [
    "fwd_return = {}\n",
    "for filing in filings:\n",
    "    date_filed = filing_index.at[filing, 'date_filed']\n",
    "    price_data = prices[prices.filing==filing].close.sort_index()\n",
    "    \n",
    "    try:\n",
    "        r = (price_data\n",
    "             .pct_change(periods=5)\n",
    "             .shift(-5)\n",
    "             .loc[:date_filed]\n",
    "             .iloc[-1])\n",
    "    except:\n",
    "        continue\n",
    "    if not np.isnan(r) and -.5 < r < 1:\n",
    "        fwd_return[filing] = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T18:33:19.711894Z",
     "start_time": "2021-02-23T18:33:19.709463Z"
    }
   },
   "outputs": [],
   "source": [
    "len(fwd_return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine returns with filing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T18:33:23.518278Z",
     "start_time": "2021-02-23T18:33:19.713296Z"
    }
   },
   "outputs": [],
   "source": [
    "y, X = [], []\n",
    "for filing_id, fwd_ret in fwd_return.items():\n",
    "    X.append(np.load(vector_path / f'{filing_id}.npy') + 2)\n",
    "    y.append(fwd_ret)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T18:33:23.521994Z",
     "start_time": "2021-02-23T18:33:23.519238Z"
    }
   },
   "outputs": [],
   "source": [
    "len(y), len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T18:33:23.546838Z",
     "start_time": "2021-02-23T18:33:23.523488Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second step, we convert the lists of integers into fixed-size arrays that we can stack and provide as input to our RNN. The pad_sequence function produces arrays of equal length, truncated, and padded to conform to maxlen, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T18:33:23.555315Z",
     "start_time": "2021-02-23T18:33:23.548429Z"
    }
   },
   "outputs": [],
   "source": [
    "maxlen = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T18:33:24.037925Z",
     "start_time": "2021-02-23T18:33:23.558144Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, \n",
    "                        truncating='pre', \n",
    "                        padding='pre', \n",
    "                        maxlen=maxlen)\n",
    "\n",
    "X_test = pad_sequences(X_test, \n",
    "                       truncating='pre', \n",
    "                       padding='pre', \n",
    "                       maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T18:33:24.041918Z",
     "start_time": "2021-02-23T18:33:24.039373Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T18:33:24.053569Z",
     "start_time": "2021-02-23T18:33:24.042819Z"
    }
   },
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define our RNN architecture. The first layer learns the word embeddings. We define the embedding dimension as previously using the input_dim keyword to set the number of tokens that we need to embed, the output_dim keyword, which defines the size of each embedding, and how long each input sequence is going to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T18:33:24.060121Z",
     "start_time": "2021-02-23T18:33:24.054841Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are using GRUs this time, which train faster and perform better on smaller data. We are also using dropout for regularization, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T18:33:24.243047Z",
     "start_time": "2021-02-23T18:33:24.061024Z"
    }
   },
   "outputs": [],
   "source": [
    "input_dim = X_train.max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T18:33:24.981202Z",
     "start_time": "2021-02-23T18:33:24.244138Z"
    }
   },
   "outputs": [],
   "source": [
    "rnn = Sequential([\n",
    "    Embedding(input_dim=input_dim, \n",
    "              output_dim=embedding_size, \n",
    "              input_length=maxlen,\n",
    "             name='EMB'),\n",
    "    BatchNormalization(name='BN1'),\n",
    "    Bidirectional(GRU(32), name='BD1'),\n",
    "    BatchNormalization(name='BN2'),\n",
    "    Dropout(.1, name='DO1'),\n",
    "    Dense(5, name='D'),\n",
    "    Dense(1, activation='linear', name='OUT')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting model has over 2 million parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T18:33:24.987696Z",
     "start_time": "2021-02-23T18:33:24.982024Z"
    }
   },
   "outputs": [],
   "source": [
    "rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T18:33:25.011631Z",
     "start_time": "2021-02-23T18:33:24.989177Z"
    }
   },
   "outputs": [],
   "source": [
    "rnn.compile(loss='mse', \n",
    "            optimizer='Adam',\n",
    "            metrics=[RootMeanSquaredError(name='RMSE'),\n",
    "                     MeanAbsoluteError(name='MAE')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T18:33:25.014276Z",
     "start_time": "2021-02-23T18:33:25.012546Z"
    }
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_MAE', \n",
    "                               patience=5,\n",
    "                               restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training stops after eight epochs and we recover the weights for the best models to find a high test AUC of 0.9346:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T19:31:03.718941Z",
     "start_time": "2021-02-23T18:33:25.015045Z"
    }
   },
   "outputs": [],
   "source": [
    "training = rnn.fit(X_train,\n",
    "                   y_train,\n",
    "                   batch_size=32,\n",
    "                   epochs=100,\n",
    "                   validation_data=(X_test, y_test),\n",
    "                   callbacks=[early_stopping],\n",
    "                   verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T19:31:03.724540Z",
     "start_time": "2021-02-23T19:31:03.720278Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(training.history)\n",
    "df.to_csv(results_path / 'rnn_sec.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T19:31:03.734144Z",
     "start_time": "2021-02-23T19:31:03.725687Z"
    }
   },
   "outputs": [],
   "source": [
    "df.index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T19:31:04.346881Z",
     "start_time": "2021-02-23T19:31:03.735490Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(14, 4), sharey=True)\n",
    "plot_data = (df[['RMSE', 'val_RMSE']].rename(columns={'RMSE': 'Training', \n",
    "                                                      'val_RMSE': 'Validation'}))\n",
    "plot_data.plot(ax=axes[0], title='Root Mean Squared Error')\n",
    "\n",
    "plot_data = (df[['MAE', 'val_MAE']].rename(columns={'MAE': 'Training', \n",
    "                                                    'val_MAE': 'Validation'}))\n",
    "plot_data.plot(ax=axes[1], title='Mean Absolute Error')\n",
    "\n",
    "for i in [0, 1]:\n",
    "    axes[i].set_xlim(1, 10)\n",
    "    axes[i].set_xlabel('Epoch')\n",
    "fig.tight_layout()\n",
    "fig.savefig(results_path / 'sec_cv_performance', dpi=300);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T19:31:24.154339Z",
     "start_time": "2021-02-23T19:31:04.349171Z"
    }
   },
   "outputs": [],
   "source": [
    "y_score = rnn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T19:31:24.166403Z",
     "start_time": "2021-02-23T19:31:24.155425Z"
    }
   },
   "outputs": [],
   "source": [
    "rho, p = spearmanr(y_score.squeeze(), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T20:01:47.141636Z",
     "start_time": "2021-02-23T20:01:47.134167Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Information Coefficient: {rho*100:.2f} ({p:.2%})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T20:01:33.087261Z",
     "start_time": "2021-02-23T20:01:32.547598Z"
    }
   },
   "outputs": [],
   "source": [
    "g = sns.jointplot(y_score.squeeze(), y_test, kind='reg');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml4t]",
   "language": "python",
   "name": "conda-env-ml4t-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
